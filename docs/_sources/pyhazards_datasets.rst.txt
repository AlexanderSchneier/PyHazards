Datasets
===================

Summary
-------

PyHazards provides a unified dataset interface for hazard prediction across tabular, temporal, and raster data. Each dataset returns a ``DataBundle`` containing splits, feature specs, label specs, and metadata.

Homogeneous Datasets
--------------------

Homogeneous graph datasets where all nodes and edges share the same feature
and label spaces. These datasets are commonly used for benchmarking node-level
and graph-level learning tasks.

.. list-table::
   :widths: 18 82
   :header-rows: 0
   :class: dataset-list

   * - ``merra2``
     - Zachary's karate club network from the *An Information Flow Model for Conflict and Fission in Small Groups* paper, containing 34 nodes, connected by 156 (undirected and unweighted) edges.
   * - ``era5``
     - A variety of graph kernel benchmark datasets, e.g., *IMDB-BINARY*, *REDDIT-BINARY* or *PROTEINS*, collected from the TU Dortmund University.
   * - ``noaa``
     - A variety of artificially and semi-artificially generated graph datasets from the *Benchmarking Graph Neural Networks* paper.
   * - ``firms``
     - The citation network datasets *Cora*, *CiteSeer* and *PubMed* from the *Revisiting Semi-Supervised Learning with Graph Embeddings* paper.
   * - ``mtbs``
     - The NELL dataset, a knowledge graph from the *Toward an Architecture for Never-Ending Language Learning* paper.
   * - ``wildfire``
     - The full citation network datasets from the *Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking* paper.

Core classes
------------

- ``Dataset``: base class to implement ``_load()`` and return a ``DataBundle``.
- ``DataBundle``: holds named ``DataSplit`` objects, plus ``feature_spec`` and ``label_spec``.
- ``FeatureSpec`` / ``LabelSpec``: describe inputs/targets to simplify model construction.
- ``register_dataset`` / ``load_dataset``: lightweight registry for discovering datasets by name.

Example skeleton
----------------

.. code-block:: python

    import torch
    from pyhazards.datasets import (
        DataBundle, DataSplit, Dataset, FeatureSpec, LabelSpec, register_dataset
    )

    class MyHazardDataset(Dataset):
        name = "my_hazard"

        def _load(self):
            x = torch.randn(1000, 16)
            y = torch.randint(0, 2, (1000,))
            splits = {
                "train": DataSplit(x[:800], y[:800]),
                "val": DataSplit(x[800:900], y[800:900]),
                "test": DataSplit(x[900:], y[900:]),
            }
            return DataBundle(
                splits=splits,
                feature_spec=FeatureSpec(input_dim=16, description="example features"),
                label_spec=LabelSpec(num_targets=2, task_type="classification"),
            )

    register_dataset(MyHazardDataset.name, MyHazardDataset)
