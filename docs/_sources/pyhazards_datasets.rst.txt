Datasets
===================

Summary
-------

PyHazards provides a unified dataset interface for hazard prediction across tabular, temporal, and raster data. Each dataset returns a ``DataBundle`` containing splits, feature specs, label specs, and metadata.

Datasets
--------------------

.. list-table::
   :widths: 18 82
   :header-rows: 0
   :class: dataset-list

   * - ``MERRA-2 (NASA GMAO)``
     - Update frequency: updated with a typical latency of about 3 weeks after the end of each month; contents: NASA global reanalysis
       (1980 to present) providing gridded meteorological fields plus land/surface fluxes and aerosol-related variables, organized into surface
       variables, vertical profile variables, and static layers or masks; tasks: drive weather and climate modeling and evaluation workflows
       (including using surface and vertical predictors as inputs to WxC-style forecasting pipelines) and support downstream natural-hazard modeling
       (e.g., wildfire or hurricane risk/impact studies) by linking hazard behavior to atmospheric and land-surface conditions; link: `MERRA-2
       overview <https://gmao.gsfc.nasa.gov/gmao-products/merra-2/>`_; reference: `Gelaro et al. (2017) <https://journals.ametsoc.org/view/journals/clim/30/14/jcli-d-16-0758.1.xml>`_.
   * - ``ERA5``
     - Update frequency: updated daily with a short-latency stream (ERA5T) and later consolidated into the final ERA5; contents: global reanalysis
       (1940 to present) with hourly fields commonly provided on a 0.25 deg x 0.25 deg latitude-longitude grid, spanning core atmospheric and
       land-surface variables such as temperature, wind, humidity, pressure, precipitation, radiation, and soil/land fields; tasks: support general
       weather and climate prediction and benchmarking, and serve as key covariates for hazard prediction and attribution studies (e.g., wildfire
       danger, heavy-rain/flooding, and hurricane-related environmental conditions) where consistent global meteorology is required; link: `ERA5
       single levels <https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels?tab=overview>`_; reference: `Hersbach et al. (2020)
       <https://rmets.onlinelibrary.wiley.com/doi/10.1002/qj.3803>`_.
   * - ``NOAA Flood Events (Storm Events Database)``
     - Update frequency: updated on a rolling basis with a typical publication delay of about 75 to 90 days after the end of the data month;
       contents: U.S. event-based hazard reports (including floods) with time and location plus impacts such as fatalities, injuries, and property/crop
       damage, rather than gridded meteorological fields; tasks: support flood impact analysis, event frequency/trend studies, and supervised modeling
       where the target is occurrence/impact (e.g., county-level flood events or damage) instead of the meteorological state itself; link: `Storm Events
       Database <https://www.ncei.noaa.gov/products/storm-events-database>`_; reference: `NCEI metadata record <https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00648>`_.
   * - ``FIRMS (Fire Information for Resource Management System)``
     - Update frequency: near-real-time publication with detections typically available within about 3 hours of satellite overpass and refreshed
       multiple times per day; contents: time-stamped active fire/hotspot detections from MODIS (about 1 km) and VIIRS (about 375 m) sensors,
       commonly used as point/cluster fire occurrence indicators; tasks: operational wildfire monitoring/nowcasting, rapid situational awareness for
       fire response, and event-level labels/targets for wildfire prediction pipelines conditioned on meteorology and fuels/land layers; link: `FIRMS
       portal <https://firms.modaps.eosdis.nasa.gov/>`_; reference: `VIIRS active fire product paper <https://doi.org/10.1016/j.rse.2013.08.008>`_.
   * - ``MTBS (Monitoring Trends in Burn Severity)``
     - Update frequency: updated on an annual production cycle with new fire-year products released after mapping is completed; contents: U.S. burn
       severity and fire perimeter products derived from Landsat at 30 m resolution with a historical record dating back to 1984; tasks: post-fire
       impact assessment, long-term wildfire regime/trend analysis, and training/validation layers for models that predict burn severity/extent or relate
       severity to meteorology, fuels, and topography; link: `MTBS portal <https://burnseverity.cr.usgs.gov/>`_; reference: `Eidenshink et al. (2007)
       <https://doi.org/10.4996/fireecology.0301003>`_.
   * - ``LANDFIRE Fuel (USFS / LANDFIRE)``
     - Update frequency: released as versioned updates and commonly treated as an annual updating program in applied workflows; contents: U.S.
       vegetation and fuels raster layers at about 30 m resolution, including vegetation type/cover/height, disturbance, and surface/canopy fuel
       descriptors for fire behavior modeling; tasks: fuels and vegetation characterization for wildfire behavior simulation, landscape-scale wildfire risk
       assessment, and static covariates for wildfire occurrence/spread models paired with dynamic meteorological predictors; link: `LANDFIRE data access
       <https://landfire.gov/getdata.php>`_; reference: `LANDFIRE program overview <https://research.fs.usda.gov/firelab/products/dataandtools/landfire-landscape-fire-and-resource-management-planning>`_.

Core classes
------------

- ``Dataset``: base class to implement ``_load()`` and return a ``DataBundle``.
- ``DataBundle``: holds named ``DataSplit`` objects, plus ``feature_spec`` and ``label_spec``.
- ``FeatureSpec`` / ``LabelSpec``: describe inputs/targets to simplify model construction.
- ``register_dataset`` / ``load_dataset``: lightweight registry for discovering datasets by name.

Example skeleton
----------------

.. code-block:: python

    import torch
    from pyhazards.datasets import (
        DataBundle, DataSplit, Dataset, FeatureSpec, LabelSpec, register_dataset
    )

    class MyHazardDataset(Dataset):
        name = "my_hazard"

        def _load(self):
            x = torch.randn(1000, 16)
            y = torch.randint(0, 2, (1000,))
            splits = {
                "train": DataSplit(x[:800], y[:800]),
                "val": DataSplit(x[800:900], y[800:900]),
                "test": DataSplit(x[900:], y[900:]),
            }
            return DataBundle(
                splits=splits,
                feature_spec=FeatureSpec(input_dim=16, description="example features"),
                label_spec=LabelSpec(num_targets=2, task_type="classification"),
            )

    register_dataset(MyHazardDataset.name, MyHazardDataset)
